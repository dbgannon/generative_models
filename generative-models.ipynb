{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models\n",
    "\n",
    "Generative Models are among the most interesting deep neural networks  and they abound with applications inscience.  There are two main types of GMs and, within each type, several interesting variations.    The important property of all generative networks is that if you train them with a sufficiently, large  and coherent collection of data samples, the network can be used to generate similar samples.   The key here is the definition of ‘coherent’.    One can say the collection is coherent if when you are presented with a new example,\n",
    "it should be a simple task to decide if it belongs to the collection or not.  For example, if the data collection consists entirely of pictures of cats, then a picture of a dog should be, with reasonably high probability, easily recognized as an outlier and not a cat.   Of course, there are always rather extreme cats that would fool most casual observers.\n",
    "In probability terms, if we think about the probability density function of cats among the space of animals $P_{cat}(X)$ would be large if X is a cat and low otherwise. Let us assume our collection $c$ is naturally represented in $R^m$ for some $m$.  For example, images with $m$ pixels or other high dimensional instrument data.  A simple way to think about a generative model is a mathematical device that transforms the multivariant normal distribution $\\mathcal{N}^k (0,1)$ into $R^m$ for our collection via a mapping\n",
    "\n",
    "$$\n",
    "Gen: v \\sim \\mathcal{N}^k (0,1) \\to R^m\n",
    "$$\n",
    "\n",
    "\n",
    "such that $P_c(Gen(v))$ is high where $P_c()$ is the probability density function for our coherent collection $c$.  \n",
    "\n",
    "A better way to say this is to insist that we have another machine we can call a discriminator $Disc$\n",
    "$$Disc: R^m \\to [0,1]$$\n",
    "\n",
    "\n",
    "such that for $x \\in R^m$, $Disc(x)$ is probability that $x$ is in the collection $c$.   In short, we would like $Disc(x)=P_c(x)$. not right!\n",
    "\n",
    "The two main types of generative neural networks are\n",
    "\n",
    "1. Generative Adversarial networks.  Introduced by Goodfellow et, al (arXiv:1406.2661). \n",
    "\n",
    "![kdnuggets](https://www.kdnuggets.com/wp-content/uploads/generative-adversarial-network.png)\n",
    "(Image from www.kdnuggets.com/wp-content/uploads/generative-adversarial-network.png)\n",
    "\n",
    "2. Variational Autoencoders\n",
    "\n",
    "#### Drug Design\n",
    "One exciting application of these techniques is in the design of drugs.   The traditional approach is high throughput screen in which large collections of chemicals are tested against potential targets to see if any have potential therapeutic effects.  Machine learning techniques have been applied to the problem for many years, but recently various deep learning method have shown surprisingly promising results.\n",
    "One of the inspirations for the recent work has been the recognition that molecular structures have properties similar to natural language (see Cadeddu, A, et. al.. Organic chemistry as a language and the implications of chemical linguistics for structural and retrosynthetic analyses. Angewandte Chemie 2014, 126.)  More specifically,  there are phrases and grammar rules in chemical compounds that have statistical properties not unlike natural language.  There is a standard string representation called SMILES that an be used to illustrate these properties.  SMILES representations describe atoms and their bonds and valences based on a depth-first tree traversal of a chemical graph.    In modern machine learning, language structure and language tasks such as machine natural language translation are aided using recurrent neural networks.  As we illustrated in our book, an RNN trained with lots of business news text is capable of generating business news headlines from a single starting word that sounds almost real.  However close inspection reveals that the content is nonsense.   However, there is no reason we cannot apply RNNs to SMILES string to see if they can generate new molecules.   Fortunately  there are sanity tests that can be applied to generated SMILES string to filter out the meaningless and incorrectly structured compounds.   This was done by a team at Novartis (Ertl et al. Generation of novel chemical matter using the LSTM neural network, arXiv:1712.07449) who demonstrated that these techniques could generate billions of new drug-like molecules.   Anvita Gupta,  Alex T. Muller, Berend J. H. Huisman,  Jens A. Fuchs, Petra Schneid and Gisbert Schneider applied very similar ideas to “Generative Recurrent Networks for De Novo Drug Design” (https://www.researchgate.net/publication/320813292/downloader).  They demonstrated that if they started with fragments of a drug of interest they could use the RNN and transfer learning to generate new variations that can may be very important.\n",
    "Shahar Harel and Kira Radinsky “Prototype-Based Compound Discovery using\n",
    "Deep Generative Models” (http://kiraradinsky.com/files/acs-accelerating-prototype.pdf )\n",
    "\n",
    "Artur Kadurin, et. al. druGAN: An Advanced Generative Adversarial Autoencoder Model for de Novo Generation of New Molecules with Desired Molecular Properties in Silico. https://pubs.acs.org/doi/10.1021/acs.molpharmaceut.7b00346\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
